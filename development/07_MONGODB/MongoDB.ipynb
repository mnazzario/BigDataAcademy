{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Vs NO-SQL\n",
    "\n",
    "What's the difference between **relational** vs. **non-relational** databases?\n",
    "\n",
    "Back in 2000 it was enough to smother your website with static text. Brochureware was cool. Not today, though. You have to have a dizzying variety of text, video, audio, images and social media to get someone's attention. But it's hard to add new content to a relational database. Or new features. Or new attributes. Not without disrupting performance or taking your database offline.\n",
    "\n",
    "With non-relational databases you can store any type of content. Incorporate any kind of data in a single database. Build any feature. Faster. With less money.\n",
    "\n",
    "### Relational (SQL)\n",
    "\n",
    "* **Stuck**. Data now includes rich data types – tweets, videos, podcasts, animated gifs – which are hard, if not impossible, to store in a relational database. Development slows to a crawl, and ops is caught playing whack-a-mole.\n",
    "\n",
    "* **Can’t Scale**. Your audience is global, in many countries, speaking many languages, accessing content on many devices. Scaling a relational database is not trivial. And it isn’t cheap.\n",
    "\n",
    "* **Expensive**. Large teams tied up for long periods of time make these applications expensive to build and maintain. Proprietary software and hardware, plus separate databases and file systems needed to manage your content, add to the cost.\n",
    "\n",
    "### Non-Relational (NoSQL)\n",
    "\n",
    "* **Do the Impossible**. NoSQL can incorporate literally any type of data, while providing all the features needed to build content-rich apps.\n",
    "\n",
    "* **Scale Big**. Scaling is built into the database. It is automatic and transparent. You can scale as your audience grows, both within a data center and across regions.\n",
    "\n",
    "* **Cheap**. More productive teams, plus commodity hardware, make your projects cost 10% what they would with a relational database.\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "Check the folder **resources** a collection of useful PDF:\n",
    "\n",
    "* *Top_5_NoSQL_Considerations.pdf*: 5 reasons to choose a NO-SQL Database\n",
    "* *AWS_&_MongoDB.pdf*: MongoDB on Amazon Web Services\n",
    "* *Mongodb_&_ApacheSpark.pdf*: How to use Apache Spark and MongoDB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB\n",
    "\n",
    " * Schema Free\n",
    " * Document Based\n",
    " * Supports Indexing\n",
    " * Not Transactional\n",
    " * Does not support relations (no JOIN)\n",
    " * Supports Autosharding\n",
    " * Automatic Replication and Failover\n",
    " * Relies on System Memory Manager\n",
    " * Has an Aggregation Pipeline\n",
    " * Builtin support for MapReduce\n",
    "\n",
    "On Python mongodb support is provided by ``PyMongo`` library, which can be installed using:\n",
    "\n",
    "\n",
    "    pip install pymongo\n",
    "    \n",
    "\n",
    "## Nexus Architecture\n",
    "\n",
    "MongoDB’s design philosophy is focused on combining the critical capabilities of relational databases (left side) with the innovations of NoSQL technologies (right side).\n",
    "\n",
    "<img src=\"nexus-architecture.png\" width=\"640\"/>\n",
    "\n",
    "\n",
    "## Installing MongoDB\n",
    "\n",
    "\n",
    "Installing MongoDB is as simple as going to http://www.mongodb.org/downloads and downloading it.\n",
    "\n",
    "Create a ``/data/db`` directory then start ``mongod`` inside the mongodb downloaded package:\n",
    "\n",
    "\n",
    "    curl -O 'https://fastdl.mongodb.org/osx/mongodb-osx-x86_64-3.0.4.tgz' \n",
    "    tar zxvf mongodb-osx-x86_64-3.0.4.tgz \n",
    "    cd mongodb-osx-x86_64-3.0.4\n",
    "    mkdir data\n",
    "    ./bin/mongod --dbpath=./data\n",
    "\n",
    "\n",
    "\n",
    "## Using MongoDB\n",
    "\n",
    "\n",
    "a ``MongoClient`` instance provides connection to MongoDB Server, each server can host multiple databases which can be retrieved with ``connection.database_name`` which can then contain multiple ``collections`` with different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = client.phonebook\n",
    "print db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the database is retrieved, collections can be accessed as attributes of the database itself.\n",
    "\n",
    "A MongoDB document is actually just a Python Dictionary, inserting a document is as simple as telling pymongo to insert the dictionary into the collection. Each document can have its own structure, can contain different data and you are not required to declare and structure of the collection. Not existing collections will be automatically created on the insertion of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {'name': 'Alex', 'phone': '+39123456789'}\n",
    "db.people.insert(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each inserted document will receive an **ObjectId** which is a uniquue identifier of the document, the ObjectId is based on some data like the current **timestamp**, **server identifier process id** and other data that guarantees it to be unique across multiple servers.\n",
    "\n",
    "Being designed to work in a distributed and multinode environment, MongoDB handles \"write safety\" by the number of servers that are expected to have saved the document before considering the insert command \"completed\".\n",
    "\n",
    "This is handled by the **w** option, which indicates the number of servers that must have saved the document before the insert command returns. Setting it to **0** makes mongodb work in *fire and forget* mode, which is useful when inserting a lot of documents quickly. As most drivers will actually generate the ObjectId on client that performs the insertion you will receive an ObjectId even before the document has been written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.people.insert({'name': 'Pippo', 'phone': '+39123456788', 'other_phone': '+3933332323'}, w=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    db.people.insert({'name': 'Jonny', 'phone': '+39123456789'}, w=2)\n",
    "except Exception as e:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching back inserted document can be done using ``find`` and ``find_one`` methods of collections. Both methods accept a query expression that filters the returned documents. Omitting it means retrieving all the documents (or in case of find_one the first document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = db.people.find_one({'name': 'Alex'})\n",
    "print res\n",
    "print type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters in mongodb are described by Documents themselves, so in case of PyMongo they are dictionaries too.\n",
    "A filter can be specified in the form ``{'field': value}``. \n",
    "By default filtering is performed by *equality* comparison, this can be changed by specifying a query operator in place of the value.\n",
    "\n",
    "Query operators by convention start with a ``$`` sign and can be specified as ``{'field': {'operator': value}}``.\n",
    "Full list of query operators is available at http://docs.mongodb.org/manual/reference/operator/query/\n",
    "\n",
    "For example if we want to find each person that has an object id greather than ``53b30ff57ab71c051823b031`` we can achieve that with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bson import ObjectId\n",
    "db.people.find_one({'_id': {'$gt':  ObjectId('5818c8549718d42038af68ad')}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Documents\n",
    "---------------------\n",
    "\n",
    "Updating documents in MongoDB can be performed with the ``update`` method of the collection. Updating is actually one of the major sources of issues for new users as it doesn't change values in document like it does on SQL based databases, but instead it replaces the document with a new one.\n",
    "\n",
    "Also note that the update operation doesn't perform update on each document identified by the query, by default only the first document is updated. To apply it to multiple documents it is required to explicitly specify the ``multi=true`` option\n",
    "\n",
    "What you usually want to do is actually using the ``$set`` operator which changes the existing document instead of replacing it with a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = db.people.find_one({'name': 'Alex'})\n",
    "print '\\nBefore Updated:', doc\n",
    "\n",
    "db.people.update({'name': 'Alex'}, {'name': 'John Doe'})\n",
    "doc = db.people.find_one({'name': 'John Doe'})\n",
    "print '\\nAfter Update:', doc\n",
    "\n",
    "# Go back to previous state\n",
    "db.people.update({'name': 'John Doe'}, {'$set': {'phone': '+39123456789'}})\n",
    "print '\\nAfter $set phone:', db.people.find_one({'name': 'John Doe'})\n",
    "db.people.update({'name': 'John Doe'}, {'$set': {'name': 'Alex'}})\n",
    "print '\\nAfter $set name:', db.people.find_one({'name': 'Alex'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SubDocuments\n",
    "--------------\n",
    "\n",
    "The real power of mongodb is released when you use subdocuments.\n",
    "\n",
    "As each mongodb document is a JSON object (actually BSON, but that doesn't change much for the user), it can contain any data which is valid in JSON. Including other documents and arrays. This replaces \"relations\" between collections in multiple use cases and it's heavily more efficient as it returns all the data in a single query instead of having to perform multiple queries to retrieve related data.\n",
    "\n",
    "As MongoDB fully supports subdocuments it is also possible to query on sub document fields and even query on arrays using the ``dot notation``.\n",
    "\n",
    "For example if you want to store a blog post in mongodb you might actually store everything, including author data and tags inside the blogpost itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.insert({'title': 'MongoDB intro!',\n",
    "                'author': {'name': 'Alex',\n",
    "                           'surname': 'Comu',\n",
    "                           'nickname': 'alexcomu'},\n",
    "                'tags': ['mongodb', 'web', 'new-hair-cut']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.find_one({'title': 'MongoDB intro!'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.find_one({'tags': 'mongodb'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.find_one({'author.name': 'Alex'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some random posts\n",
    "TAGS = ['mongodb', 'web', 'scaling', 'cooking']\n",
    "\n",
    "import random\n",
    "for postnum in range(1, 5):\n",
    "    db.blog.insert({'title': 'Post %s' % postnum,\n",
    "                    'author': {'name': 'Alex',\n",
    "                               'surname': 'Comu',\n",
    "                               'nickname': 'alexcomu'},\n",
    "                    'tags': random.sample(TAGS, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for post in db.blog.find({'tags': {'$in': ['scaling', 'cooking']}}):\n",
    "    print post['title'], '->', ', '.join(post['tags'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing\n",
    "----------\n",
    "\n",
    "Indexing is actually the most important part of MongoDB.\n",
    "\n",
    "MongoDB has great support for indexing, and it supports single key, multi key, compound and hashed indexes. Each index type has its specific use case and can be used both for querying and sorting.\n",
    "\n",
    " * Single Key -> Those are plain indexes on a field\n",
    " * Multi Key -> Those are indexes created on an array field\n",
    " * Compound -> Those are indexes that cover more than one field.\n",
    " * Hashed -> Those are indexes optimized for equality comparison, they actually store the hash of the indexed value and are usually used for sharding.\n",
    " \n",
    "In case of compound indexes they can also be used when only a part of the query filter is present into the index, there is also a special case of indexes called *covering indexes* which happen when the fields you are asking for are all available into the index. In that case MongoDB won't even access the collection and will directly serve you the data from the index. An index cannot be both a multi key index and a covering index.\n",
    "\n",
    "Indexes are also ordered, so they can be created *ASCENDING* or *DESCENDING*.\n",
    "\n",
    "Creating indexes can be done using the ``ensure_index`` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.ensure_index([('tags', 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking which index MongoDB is using to perform a query can be done using the ``explain`` method, forcing an index into a query can be done using the ``hint`` method.\n",
    "\n",
    "As MongoDB uses a statistical optimizer, using ``hint`` in queries can actually provide a performance boost as it avoids the \"best option\" lookup cost of the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.find({'tags': 'mongodb'}).explain()['queryPlanner']['winningPlan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.find({'tags': 'mongodb'}).hint([('_id', 1)]).explain()['queryPlanner']['winningPlan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.find({'title': 'Post 1'}).explain()['queryPlanner']['winningPlan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.blog.ensure_index([('author.name', 1), ('title', 1)])\n",
    "db.blog.find({'author.name': 'Alex'}, {'title': True, '_id': False}).explain()['queryPlanner']['winningPlan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Atleti DB\n",
    "\n",
    "Here you can find a simple mongo Connector used to fill a db \"olimpiadi\" with a collection named \"atleti\" that contains all the atlethes from the CSV source file.\n",
    "\n",
    "To run this script you can simply run from a terminal: \n",
    "    \n",
    "    python import_athletes.py athletes_sochi.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "db = client.olimpiadi\n",
    "\n",
    "# DATA FORMAT\n",
    "# age,birthdate,gender,height,name,weight,\n",
    "# gold_medals,silver_medals,bronze_medals,\n",
    "# total_medals,sport,country\n",
    "\n",
    "with open('athletes_sochi.csv', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        splitted = line.split(',')\n",
    "        if splitted[0] != 'age':\n",
    "            data = {'age':splitted[0],\n",
    "                'birthdate':splitted[1],\n",
    "                'gender':splitted[2],\n",
    "                'height':splitted[3],\n",
    "                'name':splitted[4],\n",
    "                'weight':splitted[5],\n",
    "                'gold_medals':int(splitted[6]),\n",
    "                'silver_medals':int(splitted[7]),\n",
    "                'bronze_medals':int(splitted[8]),\n",
    "                'total_medals':int(splitted[9]),\n",
    "                'sport':splitted[10],\n",
    "                'country':splitted[11][:-2]}\n",
    "            db.atleti.insert(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo IMPORT / EXPORT\n",
    "\n",
    "Instead to use the python script we can also use **MONGOIMPORT** provided by MongoDB itself:\n",
    "\n",
    "    mongoimport --db olimpiadi --collection atleti --file atleti.json\n",
    "\n",
    "To export the entire collection we can use **MONGOEXPORT**:\n",
    "\n",
    "    mongoexport --db olimpiadi --collection atleti --out atleti.json\n",
    "    \n",
    "Short version of *mongoexport*:\n",
    "    \n",
    "    mongoexport -d olimpiadi -c atleti -o atleti.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation Pipeline\n",
    "----------------------\n",
    "\n",
    "The aggreation pipeline provided by the aggreation framework is a powerful feature in MongoDB that permits to perform complex data analysis by passing the documents through a pipeline of operations.\n",
    "\n",
    "MongoDB was created with the cover philosophy that you are going to store your documents depending on the way you are going to read them. So to properly design your schema you need to know how you are going to use the documents. While this approach provides great performance benefits and is more concrete in case of web application, it might not always be feasible.\n",
    "\n",
    "In case you need to perform some kind of analysis your documents are not optimized for, you can rely on the aggreation framework to create a pipeline that transforms them in a way more practical for the kind of analysis you need.\n",
    "\n",
    "### How it works\n",
    "\n",
    "The aggregation pipeline is a list of operations that gets executed one after the other on the documents of the collections. The first operation will be performed on all the documents, while successive operations are performed on the result of the previous steps.\n",
    "\n",
    "If steps are able to take advantage of **indexes** they will, that is the case for a **match** or **sort** operator, if it appears at the begin of the pipeline. All operators start with a <span><strong>$</strong></span> sign\n",
    "\n",
    "### Stage Operators\n",
    "\n",
    "\n",
    "* **project**\tReshapes each document in the stream, such as by adding new fields or removing existing fields. For each input document, outputs one document.\n",
    "* **match**\tFilters the document stream to allow only matching documents to pass unmodified into the next pipeline stage. **match** uses standard MongoDB queries. For each input document, outputs either one document (a match) or zero documents (no match).\n",
    "* **limit**\tPasses the first n documents unmodified to the pipeline where n is the specified limit. For each input document, outputs either one document (for the first n documents) or zero documents (after the first n documents).\n",
    "* **skip**\tSkips the first n documents where n is the specified skip number and passes the remaining documents unmodified to the pipeline. For each input document, outputs either zero documents (for the first n documents) or one document (if after the first n documents).\n",
    "* **unwind**\tDeconstructs an array field from the input documents to output a document for each element. Each output document replaces the array with an element value. For each input document, outputs n documents where n is the number of array elements and can be zero for an empty array.\n",
    "* **group**\tGroups input documents by a specified identifier expression and applies the accumulator expression(s), if specified, to each group. Consumes all input documents and outputs one document per each distinct group. The output documents only contain the identifier field and, if specified, accumulated fields.\n",
    "* **sort**\tReorders the document stream by a specified sort key. Only the order changes; the documents remain unmodified. For each input document, outputs one document.\n",
    "* **geoNear**\tReturns an ordered stream of documents based on the proximity to a geospatial point. Incorporates the functionality of **match**, **sort**, and **limit** for geospatial data. The output documents include an additional distance field and can include a location identifier field.\n",
    "* **out**\tWrites the resulting documents of the aggregation pipeline to a collection. To use the $out stage, it must be the last stage in the pipeline.\n",
    "\n",
    "#### Expression Operators\n",
    "\n",
    "Each stage operator can work with one or more **expression operator** which allow to perform actions during that stage, for a list of expression operators see http://docs.mongodb.org/manual/reference/operator/aggregation/#expression-operators\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = client.olimpiadi\n",
    "\n",
    "# How many people from Italy and France?\n",
    "print len(list(db.atleti.aggregate([\n",
    "                {'$match': {'country': {'$in':['Italy', 'France']}}}\n",
    "            ])))\n",
    "\n",
    "print \"\\n-----\\n\"\n",
    "\n",
    "# Count them using only the pipeline \n",
    "print db.atleti.aggregate([\n",
    "        {'$match': {'country': {'$in':['Italy', 'France']}}},\n",
    "        {'$group': {'_id': 'count', 'count': {'$sum': 1}}}\n",
    "    ]).next()\n",
    "\n",
    "print \"\\n-----\\n\"\n",
    "\n",
    "# Count people with at least 10 medals\n",
    "print list(db.atleti.aggregate([\n",
    "        {'$project': {'country': 1, 'total_medals': 1, '_id': 0}},\n",
    "        {'$group': {'_id': '$country', 'medals': {'$sum': '$total_medals'}}},\n",
    "        {'$match': {'medals': {'$gt': 20}}},\n",
    "        {'$sort': {'medals': 1}}\n",
    "    ]))\n",
    "\n",
    "\n",
    "print \"\\n-----\\n\"\n",
    "\n",
    "# The same as before\n",
    "country_with_more_than_ten_medals = db.atleti.aggregate([\n",
    "        {'$project': {'country': 1, 'total_medals': 1, '_id': 0}},\n",
    "        {'$group': {'_id': '$country', 'medals': {'$sum': '$total_medals'}}},\n",
    "        {'$match': {'medals': {'$gt': 20}}},\n",
    "        {'$sort': {'medals': 1}}\n",
    "    ])\n",
    "for entry in country_with_more_than_ten_medals:\n",
    "    print entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce\n",
    "----------\n",
    "\n",
    "MongoDB is powered by the V8 javascript engine, this means that each mongod node is able to run javascript code.\n",
    "With an high enough number of mongod nodes, you actually end up with a powerful execution environment for distributed code that also copes with the major problem of data locality.\n",
    "\n",
    "For this reason MongoDB exposes a **mapreduce** function which can be leveraged in shareded environments to run map reduce jobs.\n",
    "Note that the Aggregation Pipeline is usually faster than the mapReduce feature, and it scales with the number of nodes as mapReduce, so you should rely on MapReduce only when the algorithm cannot be efficiently expressed with the Aggregation Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.atleti.map_reduce(map='''function(){\n",
    "            var country = this.country;\n",
    "            var medals = parseInt(this.total_medals);\n",
    "            emit(country, medals);\n",
    "        }''', reduce='''function(key, values){\n",
    "            return Array.sum(values)\n",
    "        }''',out='medalsfrequency')\n",
    "\n",
    "for entry in db.medalsfrequency.find().sort('value'):\n",
    "    if entry['value'] > 0:\n",
    "        print entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio 1\n",
    "\n",
    "Utilizzando il dataset, calcolare per ogni età il numero di atleti.\n",
    "\n",
    "## Esercizio 2\n",
    "\n",
    "Partendo dall'esercizio precedente, restituire come output l'eta' con il numero maggiore di occorrenze.\n",
    "\n",
    "## Esercizio 3\n",
    "\n",
    "Calcolare il numero di uomini e donne per ogni eta'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Esercizio 1\n",
    "\n",
    "db.atleti.map_reduce(map='''function(){\n",
    "            emit(this.age, 1);\n",
    "        }''', reduce='''function(key, values){\n",
    "            return Array.sum(values)\n",
    "        }''',out='agefrequency')\n",
    "\n",
    "for entry in db.agefrequency.find().sort('id'):\n",
    "    print entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Esercizio 2\n",
    "\n",
    "db.atleti.map_reduce(map='''function(){\n",
    "            emit(this.age, 1);\n",
    "        }''', reduce='''function(key, values){\n",
    "            return Array.sum(values)\n",
    "        }''',out='agefrequency')\n",
    "\n",
    "max(db.agefrequency.find(), key=lambda x:x['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Esercizio 3\n",
    "\n",
    "db.atleti.map_reduce(map='''function(){\n",
    "            emit(this.gender + \" \" + this.age, 1);\n",
    "        }''', reduce='''function(key, values){\n",
    "            return Array.sum(values)\n",
    "        }''',out='genderfrequency')\n",
    "\n",
    "for entry in db.genderfrequency.find().sort('id'):\n",
    "    if entry['value'] > 10:\n",
    "        print entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Scaling - Sharding\n",
    "====================\n",
    "\n",
    "Sharding, or horizontal scaling, divides the data set and distributes the data over multiple servers, or shards. Each shard is an independent database, and collectively, the shards make up a single logical database.\n",
    "\n",
    "**Chunk**\n",
    "The whole set of data is divided in Chunks, chunk are then distributed as equally as possible through all the nodes\n",
    "\n",
    "**Shard Key**\n",
    "The shard key is the *Document* property on which chunks are decided, the range of shard key possible values is divided in chunks and each chunk is assigned to a node. Document which near values for the shard key will end up being in the same chunk and so on the same node.\n",
    "\n",
    "**Shard**\n",
    "Each MongoDB node or ReplicaSet that contains part of the sharded data.\n",
    "\n",
    "**Router**\n",
    "The routers is the interface to the cluster, each query and operation will be performed against the router. The router is then in charge of forwarding the operation to one or multiple shards and gather the results.\n",
    "\n",
    "**Config Server**\n",
    "The config servers keep track of chunks distribution, they know which shard contains which chunk and which values are kept inside each chunk. Whenever the router has to perform an operation or split chunks that became too big it will read and write chunks distribution from the config servers.\n",
    "\n",
    "Setting Up a Sharder Cluster\n",
    "================================================\n",
    "\n",
    "To properly setup a shared environment I suggest you to check [THIS REPOSITORY](https://github.com/alexcomu/mongodb_howto). Is a collection of *How To* for:\n",
    "\n",
    "___01 - Install & Play with MongoDB___\n",
    "\n",
    "___02 - Replica Set tutorial___\n",
    "\n",
    "___03 - Arbiter configuration___\n",
    "\n",
    "___04 - Sharding___\n",
    "\n",
    "___05 - From Replica Set to Sharding - Tutorial___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Map Reduce and MongoDB\n",
    "\n",
    "# Esercizio\n",
    "\n",
    "Utilizzare il dataset FakeFriends del capitolo 05 per creare uno script MapReduce che come output del processo vada a scrivere all'interno di MongoDB una entry per ogni riga del dataset. La entry dovrà contenere tutte le informazioni presenti all'interno del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
