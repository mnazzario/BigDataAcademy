{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS SETUP\n",
    "\n",
    "We need credentials to access to AWS resources.\n",
    "\n",
    "<img src=\"img1.png\" width=\"400px\">\n",
    "\n",
    "Click on **Access Keys (Access Key ID and Secret Access Key)** and generate a new Key (**REMEMBER** to save the credentials!).\n",
    "\n",
    "<img src=\"img2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Running on EMR\n",
    "------------------\n",
    "\n",
    "By default MrJob runs the job on your own computer using the ``multiprocessing`` module.\n",
    "To change this behavior and make it run the processes on amazon EMR we need to specify a **\"mrjob.conf\"** config file and let MrJob load the configuration from it.\n",
    "\n",
    "A typical MrJob configuration file for EMR looks like:\n",
    "\n",
    "```Â \n",
    "runners:\n",
    "  emr:\n",
    "      region: eu-west-1\n",
    "      aws_access_key_id: KEY\n",
    "      aws_secret_access_key: SECRET_KEY\n",
    "      num_core_instances: 4\n",
    "      instance_type: c1.medium\n",
    "      ec2_key_pair: EMR\n",
    "      ec2_key_pair_file: ~/.ssh/EMR.pem \n",
    "      ssh_tunnel: true\n",
    "```\n",
    "\n",
    "To **run** the wordcount on EMR we need to tell MrJob to use EMR and which configuration file to load:\n",
    "\n",
    "```\n",
    "$ MRJOB_CONF=./mrjob.conf python wordcount.py -r emr lorem.txt\n",
    "```\n",
    "\n",
    "### EMR Startup Steps ###\n",
    "\n",
    "First step performed by MrJob is loading your configuration file:\n",
    "\n",
    "```\n",
    "using configs in ./mrjob.conf\n",
    "```\n",
    "\n",
    "Then it will create an S3 bucket where the *wordcount.py* and the data is uploaded (lorem.txt)\n",
    "\n",
    "```\n",
    "(envCourses) MBP-di-Alex:examples alexcomu$ MRJOB_CONF=./mrjob.conf python firstletter_count.py -r emr lorem.txt \n",
    "Using configs in ./mrjob.conf\n",
    "Auto-created temp S3 bucket mrjob-adfa692a01d6eae0\n",
    "Using s3://mrjob-adfa692a01d6eae0/tmp/ as our temp dir on S3\n",
    "Creating temp directory /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/solution_03_ex1.alexcomu.20160628.212525.574013\n",
    "Copying local files to s3://mrjob-adfa692a01d6eae0/tmp/solution_03_ex1.alexcomu.20160628.212525.574013/files/...\n",
    "Created new cluster j-1WM54P4L1AF4F\n",
    "Waiting for step 1 of 1 (s-1T3DC5OWYK99Z) to complete...\n",
    "\n",
    "```\n",
    "\n",
    "Now that the data is available it will start the EMR job and the required EC2 machines:\n",
    "\n",
    "```\n",
    "PENDING (cluster is STARTING)\n",
    "PENDING (cluster is STARTING)\n",
    "...\n",
    "PENDING (cluster is STARTING)\n",
    "PENDING (cluster is STARTING: Configuring cluster software)\n",
    "PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
    "PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
    "RUNNING for 13.7s\n",
    "RUNNING for 44.1s\n",
    "...\n",
    "\n",
    "Attempting to fetch counters from logs...\n",
    "Waiting for cluster (j-1WM54P4L1AF4F) to terminate...\n",
    "  TERMINATING: Steps completed\n",
    "  TERMINATED: Steps completed\n",
    "Looking for step log in s3://mrjob-adfa692a01d6eae0/tmp/logs/j-1WM54P4L1AF4F/steps/s-1T3DC5OWYK99Z...\n",
    "  Parsing step log: s3://mrjob-adfa692a01d6eae0/tmp/logs/j-1WM54P4L1AF4F/steps/s-1T3DC5OWYK99Z/syslog.gz  \n",
    "Removing s3 temp directory s3://mrjob-adfa692a01d6eae0/tmp/solution_03_ex1.alexcomu.20160628.212525.574013/...\n",
    "Removing temp directory /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/solution_03_ex1.alexcomu.20160628.212525.574013...\n",
    "Removing log files in s3://mrjob-adfa692a01d6eae0/tmp/logs/j-1WM54P4L1AF4F/...\n",
    "Terminating cluster: j-1WM54P4L1AF4F\n",
    "```\n",
    "\n",
    "After ~5 minutes the job completed (on local it took ~10s, this is why we will test most jobs locally) and MrJob will download the output from S3:\n",
    "\n",
    "```\n",
    "Streaming final output from s3://mrjob-adfa692a01d6eae0/tmp/solution_03_ex1.alexcomu.20160628.212525.574013/output/...\n",
    "\"a\"\t20\n",
    "\"b\"\t1\n",
    "\"c\"\t6\n",
    "\"d\"\t2\n",
    "\"e\"\t11\n",
    "\"f\"\t5\n",
    "\"h\"\t1\n",
    "\"i\"\t9\n",
    "\"l\"\t11\n",
    "\"m\"\t8\n",
    "\"n\"\t13\n",
    "\"o\"\t1\n",
    "\"p\"\t12\n",
    "\"q\"\t5\n",
    "\"r\"\t1\n",
    "\"s\"\t10\n",
    "\"t\"\t6\n",
    "\"u\"\t7\n",
    "\"v\"\t11\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running on BIGDATA\n",
    "\n",
    "When testing on EMR we have let MrJob configure an EMR cluster for us and upload the data files,\n",
    "while this might be good for spot tries it is not feasible when working on big data that require multiple EMR machine that have to be configured each time and big data files that have to be copied to S3 each time.\n",
    "\n",
    "To avoid this issue MrJob permits to run the code against an existing EMR cluster and S3 bucket.\n",
    "\n",
    "## Running on existing data\n",
    "\n",
    "\n",
    "To run on data already on S3, you can pass one or multiple ``s3://`` url to use as data sources.\n",
    "For example if we want to run a job against the Twitter Dataset provided for the team work exercise we can do that using:\n",
    "\n",
    "```\n",
    "$ MRJOB_CONF=./mrjob.conf python myscript.py -r emr s3://alexcomu/berlinale_aggregated.csv\n",
    "```\n",
    "\n",
    "**When using S3 pay attention to the AWS region**, the s3 bucket region must be the same written inside your **mrjob.conf**, so in this case we must ensure that ``aws_region: eu-west-1`` is specified inside the configuration file, or accessing to the dataset will fail.\n",
    "\n",
    "**NOTE:** Hadoop is able to use GZIP compressed files for input, so nothing particular is required to work with  *2011-02-11.json.XY.gz* files, they will be automatically decompressed by hadoop itself and you will get the contained JSON as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs on EMR from WebApps and Runners\n",
    "\n",
    "We have already seen that when running JOBS from webapps or other python scripts RUNNERS must be used. So far we only used runners by providing the input ourselves and running them locally.\n",
    "When we want to run the job on EMR and we want to load data from an S3 bucket we must pass the options to the runner like we would by command line.\n",
    "Specifically in case we want to run a WORD COUNT job on the twitter dataset the runner might look like:\n",
    "\n",
    "## Example of application\n",
    "\n",
    "We want to analyze the file https://s3-eu-west-1.amazonaws.com/alexcomu/berlinale_aggregated.csv which is composed by:\n",
    "```\n",
    "berlinale1;1454331524;995823735;213.61.32.110;10;1580;0;wowza02;instance1;wowza_app;\n",
    "app-name;timestamp;session-id;client-IP;seconds;kbytes;client-type;server-name;instance;stream-name;\n",
    "``` \n",
    "Let's try to sum amount of kbytes transmitted per app-name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "class MRBerlinaleKBytes(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        (app_name, timestamp_start, session_id,\n",
    "         client_ip, length_stream, kbyte_transf,\n",
    "         client_type, server_name, wowza_instance, stream_name) = line.split(\";\")\n",
    "        yield app_name, int(kbyte_transf)\n",
    "\n",
    "\n",
    "    def reducer(self, app_name, kbytes):\n",
    "        yield app_name, sum(kbytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from berlinale import MRBerlinaleKBytes\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "INPUT_FILE = 's3://alexcomu/berlinale_aggregated.csv'\n",
    "\n",
    "mr_job = MRBerlinaleKBytes(args=['-r', 'emr', \n",
    "                                '--conf-path', 'mrjob.conf',\n",
    "                                INPUT_FILE])\n",
    "\n",
    "output = {}\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        key, value = mr_job.parse_output_line(line)\n",
    "        output[key] = value\n",
    "\n",
    "import json\n",
    "print json.dumps(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the application:\n",
    "\n",
    "     python runner.py \n",
    "\n",
    "Here the output\n",
    "\n",
    "```\n",
    "(envCourses) MBP-di-Alex:00_example alexcomu$ python runner.py \n",
    "INFO:mrjob.emr:Using s3://mrjob-adfa692a01d6eae0/tmp/ as our temp dir on S3\n",
    "INFO:mrjob.runner:Creating temp directory /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/berlinale.alexcomu.20160628.215720.741936\n",
    "INFO:mrjob.emr:Copying local files to s3://mrjob-adfa692a01d6eae0/tmp/berlinale.alexcomu.20160628.215720.741936/files/...\n",
    "INFO:mrjob.emr:Created new cluster j-69OH7QX323LS\n",
    "INFO:mrjob.emr:Waiting for step 1 of 1 (s-37CO88BBCWNZT) to complete...\n",
    "INFO:mrjob.emr:  PENDING (cluster is STARTING)\n",
    "INFO:mrjob.emr:  PENDING (cluster is STARTING)\n",
    "...\n",
    "INFO:mrjob.emr:  PENDING (cluster is STARTING)\n",
    "INFO:mrjob.emr:  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
    "...\n",
    "INFO:mrjob.emr:  RUNNING for 7.0s\n",
    "INFO:mrjob.emr:  RUNNING for 38.1s\n",
    "...\n",
    "INFO:mrjob.emr:  RUNNING for 174.5s\n",
    "INFO:mrjob.emr:  COMPLETED\n",
    "INFO:mrjob.logs.mixin:Attempting to fetch counters from logs...\n",
    "INFO:mrjob.emr:Waiting for cluster (j-2SL7Q218ZQ2XW) to terminate...\n",
    "INFO:mrjob.emr:  TERMINATING: Steps completed\n",
    "INFO:mrjob.emr:  TERMINATED: Steps completed\n",
    "INFO:mrjob.emr:Looking for step log in s3://mrjob-adfa692a01d6eae0/tmp/logs/j-2SL7Q218ZQ2XW/steps/s-35VOMK863GN4Z...\n",
    "INFO:mrjob.emr:  Parsing step log: s3://mrjob-adfa692a01d6eae0/tmp/logs/j-2SL7Q218ZQ2XW/steps/s-35VOMK863GN4Z/syslog.gz\n",
    "...\n",
    "...\n",
    "...\n",
    "```\n",
    "\n",
    "The result is:\n",
    "```\n",
    "{\"berlinale_tc1\": 72637117, \"berlinale_ext_pke\": 400595652, \"berlinale_film\": 1411678707, \"berlinale_rtd\": 8916633097, \"bal_berlinale_rt_low\": 6394, \"berlinale_rte\": 8654319443, \"berlinale_pke\": 10233396499, \"berlinale_pkd\": 9994392877, \"berlinale_prt_pkd\": 5175364, \"berlinale_prt_pke\": 6498380, \"berlinale_enc\": 847054727, \"berlinale_ext_pkd\": 573904214, \"berlinale_pk_low\": 15049, \"berlinale_tc2\": 13644927, \"berlinale_rt\": 4783701218}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if I want run my script on AWS and use S3 as output I can simply run:\n",
    "\n",
    "    $ MRJOB_CONF=./mrjob.conf python berlinale.py -r emr -v s3://alexcomu/berlinale_aggregated.csv  --output-dir=s3://YOUR-BUCKET/DESTINATION-FOLDER\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
