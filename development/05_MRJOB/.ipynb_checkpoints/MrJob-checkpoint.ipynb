{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MrJob\n",
    "\n",
    "MrJob è un framework MapReduce in grado di essere eseguito su:\n",
    "\n",
    " - Locally using multiprocessing\n",
    " - Hadoop Cluster\n",
    " - Amazon EMR\n",
    " \n",
    "Si tratta di una potente astrazione fra cluster che si occupa di gestire il file system dell'infrastruttura e nodi che si occupano di processare i dati. Non si dovrà apportare nessuna modifica al codiche che scriveremo se passeremo dall'ambiente locale, a Hadoop o EMR.\n",
    "\n",
    "MrJob può essere installato usando:\n",
    "    \n",
    "    $ pip install mrjob\n",
    "    \n",
    "nel vostro environment python.\n",
    "\n",
    "## Protocols\n",
    "\n",
    "Grazie ai **Protocols** implementati nel framework, sarà possibile in maniera semplice gestire l'I/O come richiesto Hadoop Streaming. Ogni qualvolta si creerà un processo MrJob si potrà andare a definire encode/decode dei dati attravarso i protocolli ``RawValueProtocol`` (che sfrutta lo standard dei protocolli Hadoop) o ``JSONProtocol`` che utilizza un JSON Encoding per rappresentare dati piu complessi rispetto a semplice testo.\n",
    "\n",
    "## MrJob Steps\n",
    "\n",
    "\n",
    "MrJob da la possibilità di creare steps multipli anzichè il classico singolo step MapReduce.\n",
    "\n",
    "Lo step più classico è il cosidetto **combiner** che viene eseguito subito dopo il mapper ma prima del reducer. Questo step può combinare valori multipli emessi da diversi mapper in un singolo output. Solitamente è identico allo step del **reducer** ma viene eseguito solo un subset degli output dei mapper anzichè da tutti.\n",
    "\n",
    "\n",
    "Ecco la lista di tutti i possibili steps:\n",
    "\n",
    "### Main Steps ###\n",
    "- mapper()\n",
    "- combiner() --> permette su una stessa macchina inizia a fare l'operazione di reduce senza aspettare tutti\n",
    "- reducer()\n",
    "\n",
    "### Initialization ### servono per fare operazione prima della map reduce\n",
    "- mapper_init()\n",
    "- combiner_init()\n",
    "- reducer_init()\n",
    "\n",
    "### Finalization ### servono per fare operazione dopo la map reduce\n",
    "- mapper_final()\n",
    "- combiner_final()\n",
    "- reducer_final()\n",
    "\n",
    "### Filtering ###\n",
    "- mapper_pre_filter()\n",
    "- combiner_pre_filter()\n",
    "- reducer_pre_filter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MrJob In Practice\n",
    "\n",
    "Per creare un processo MrJob sarà necessario definire una classe che eredità da ``mrjob.job.MRJob`` e dichiarare i metodi necessari per lo step MapReduce. Ogni metodo deve *emettere* una coppia chiave, valore tramite l'uso di ``yield``.\n",
    "\n",
    "Questo serve per far si che ogni step MapReduce non debba ricordare il valore emesso, funziona come generatore che restituisce ogni volta un nuovo valore allo step successivo ogni qual volta viene richiamato.\n",
    "\n",
    "Siccome si sfrutta il paradigma *Hadoop Streaming* ogni processo MrJob dovrà essere un modulo python in grado di partire da linea di comando. In Python per realizzare uno script avviabile da terminale si può utilizzare la seguente best practice:\n",
    "\n",
    "```\n",
    "if __name__ == '__main__':\n",
    "   do_something()\n",
    "```\n",
    "\n",
    "Grazie a questo snippet di codice si renderà eseguibile il fie (``__name__`` è uguale a __main__ solo quando lo script python .py è eseguito da linea di comando). Di conseguenza quando lo script verrà eseguito da linea di comando la funzione ``do_something()`` verrà eseguita.\n",
    "\n",
    "Nel caso di un processo MrJob dovremo sostituire il *do_something()* con ``MrJobProcessName.run()`` per far partire il nostro processo.\n",
    "\n",
    "La ragione di tutto questo è perchè il nostro file **.py** sarà copiato sul cluster Hadoop per essere eseguito e quindi deve poter essere eseguito come standalone software per essere compatibile con Hadoop Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCount \n",
    "\n",
    "Ecco un primo esempio di WordCount in \"pure python\". Dato un testo contiamo le occorrenze di ogni parola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_text = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque molestie lacus a iaculis tempus.\\\n",
    "Nam lorem nulla, viverra non pulvinar ut, fermentum et tortor. Cras vitae libero sed purus venenatis posuere. \\\n",
    "Proin commodo risus augue, vitae suscipit lectus accumsan sit amet. Praesent eu erat sem. \\\n",
    "Pellentesque interdum porta libero, et ultrices nunc eleifend sit amet. \\\n",
    "In in mauris nec elit ullamcorper ultrices at ac ante. Suspendisse potenti. \\\n",
    "Aenean eu nisl in ante adipiscing imperdiet. Ut pulvinar lectus quis feugiat adipiscing. \\\n",
    "Nunc vulputate mauris congue diam ultrices aliquet. Nulla pharetra laoreet est quis vestibulum. \\\n",
    "Quisque feugiat pharetra sagittis. Phasellus nulla massa, sodales a suscipit blandit, facilisis eu augue. \\\n",
    "Cras mi massa, ullamcorper nec tristique at, convallis quis eros. \\\n",
    "Mauris non fermentum lacus, vitae tristique tellus. In volutpat metus augue, nec laoreet ante hendrerit vitae. \\\n",
    "Vivamus id lacus nec orci tristique vulputate.\n",
    "'''\n",
    "\n",
    "# Usiamo una Regular Expression per identificare le parole\n",
    "import re\n",
    "WORD_REGEXP = re.compile(r\"[\\w']+\")\n",
    "\n",
    "def wordCount(input, result):\n",
    "    '''\n",
    "    Funzione per calcolare le occorrenze di parole in un testo\n",
    "    PARAM:\n",
    "        input: Testo su cui cercare le parole\n",
    "        result: Dizionario in cui scrivere il risultato\n",
    "    '''\n",
    "    # Cerchiamo tutte le words\n",
    "    words = WORD_REGEXP.findall(input)\n",
    "    # Per ogni Word\n",
    "    for word in words:\n",
    "        # Verificare se abbiamo gia contato occorrenze di quella parola\n",
    "        if word not in result:\n",
    "            # Parola non ancora analizzata-> Init chiave nel dizionario\n",
    "            result[word] = 0\n",
    "        # Somma dell'occorrenza per la parola specifica\n",
    "        result[word] += 1\n",
    "\n",
    "# Dizionario su cui scriveremo i risultati\n",
    "result = {}   \n",
    "# Chiamo la funzione\n",
    "wordCount(my_text, result)\n",
    "# Stampo per ogni parola il numero di occorrenze\n",
    "for word, occurences in result.items():\n",
    "    print word, occurences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risolviamo lo stesso esercizio utilizzando MrJob (**Attenzione**: eseguire lo script tramite linea di comando)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importiamo la classe MrJob\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "WORD_REGEXP = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# Inizializziamo la classe MRWordFreqCount che eredita da MRJob\n",
    "class MRWordFreqCount(MRJob):\n",
    "    \n",
    "    # Dichiariamo il mapper\n",
    "    def mapper(self, _, line):\n",
    "        # Nel nostro caso non avremo una chiave ma solo un value, che corrispondera a una riga del testo in input\n",
    "        words = WORD_REGEXP.findall(line)\n",
    "        for word in words:\n",
    "            # Per ogni parola restituiamo la parola in minuscolo e il numero di occorrenze, ovviamente 1\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    # Dichiariamo il reducer\n",
    "    def reducer(self, word, counts):\n",
    "        # Per ogni parola contiamo il numero delle occorrenze\n",
    "        yield word, sum(counts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        MRWordFreqCount.run()\n",
    "    except TypeError:\n",
    "        print 'MrJob cannot work inside iPython Notebook as it is not saved as a standalone .py file'\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting MrJob\n",
    "\n",
    "Supponendo di aver salvato il precedente script in un file **wordcount.py** si può eseguire lo script da terminale:\n",
    "\n",
    "```\n",
    "$ python wordcount.py lorem.txt\n",
    "```\n",
    "\n",
    "Dove lorem.txt è un file di testo contenente il testo usato in input:\n",
    "\n",
    "```\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque molestie lacus a iaculis tempus. Nam lorem nulla, viverra non pulvinar ut, fermentum et tortor. Cras vitae libero sed purus venenatis posuere. Proin commodo risus augue, vitae suscipit lectus accumsan sit amet. Praesent eu erat sem. Pellentesque interdum porta libero, et ultrices nunc eleifend sit amet. In in mauris nec elit ullamcorper ultrices at ac ante. Suspendisse potenti. Aenean eu nisl in ante adipiscing imperdiet. Ut pulvinar lectus quis feugiat adipiscing.\n",
    "Nunc vulputate mauris congue diam ultrices aliquet. Nulla pharetra laoreet est quis vestibulum. Quisque feugiat pharetra sagittis. Phasellus nulla massa, sodales a suscipit blandit, facilisis eu augue. Cras mi massa, ullamcorper nec tristique at, convallis quis eros. Mauris non fermentum lacus, vitae tristique tellus. In volutpat metus augue, nec laoreet ante hendrerit vitae. Vivamus id lacus nec orci tristique vulputate.\n",
    "```\n",
    "\n",
    "Quando si esegue lo script vedremo qualcosa del tipo:\n",
    "\n",
    "```\n",
    "Creating temp directory /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/script.alexcomu.20161010.152629.080892\n",
    "Running step 1 of 1...\n",
    "\n",
    "```\n",
    "\n",
    "E l'output finale:\n",
    "\n",
    "```\n",
    "Streaming final output from /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/script.alexcomu.20161010.152629.080892/output...\n",
    "\"a\"\t2\n",
    "\"ac\"\t1\n",
    "\"accumsan\"\t1\n",
    "\"adipiscing\"\t3\n",
    "\"aenean\"\t1\n",
    "\"aliquet.\"\t1\n",
    "```\n",
    "\n",
    "Da notare che MRJob si occupa di copiare input e output dei vari step all'interno di cartelle temporanee (in questo caso sul mio pc siccome non ho eseguito lo script su un cluster hadoop) e poi di rispondere con l'output.\n",
    "\n",
    "Si può inoltre notare che l'encoding dell'output è nel formato Hadoop Streaming, le parole e il numero di occorrenze sono separate fra loro da un carattere TAB \"\\t\"\n",
    "\n",
    "## Aiuto\n",
    "\n",
    "Per salvare in automatico il risultato all'interno di un file possiamo usare il semplice redirect shell:\n",
    "\n",
    "```\n",
    "$ python wordcount.py lorem.txt > result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosa può essere emesso?\n",
    "\n",
    "Al momento stiamo contando le parole, cioè il classico esempio per spiegare come funziona MapReduce, ma cosa capita se vogliamo emettere un output diverso che non derivi strettamente dall'input?\n",
    "\n",
    "MapReduce non fa nessuna assunzione sul dato emesso, ne le chiavi ne i valori devono essere correlati all'input in nessun modo. Per esempio, il mapper può restituire qualsiasi dato esso voglia.\n",
    "\n",
    "Quindi, se vogliamo calcolare qualche statistica sul test come parole, caratteri e frasi, possiamo farlo molto semplicemente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import re\n",
    "WORD_REGEXP = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRTextInfo(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for phrase in line.split('.'):\n",
    "            yield 'phrases', 1\n",
    "            words = WORD_REGEXP.findall(phrase)\n",
    "            for word in words:\n",
    "                yield 'words', 1\n",
    "                yield 'characters', len(word)\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        MRTextInfo.run()\n",
    "    except TypeError:\n",
    "        print 'MrJob cannot work inside iPython Notebook as it is not saved as a standalone .py file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'output sarà il seguente:\n",
    "\n",
    "Creating temp directory /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/script.alexcomu.20161010.153948.901276\n",
    "\n",
    "Running step 1 of 1...\n",
    "\n",
    "Streaming final output from /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/script.alexcomu.20161010.153948.901276/output...\n",
    "\n",
    "    \"characters\"\t791\n",
    "    \"phrases\"\t39\n",
    "    \"words\"\t140\n",
    "\n",
    "Removing temp directory /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/script.alexcomu.20161010.153948.901276...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiStep Jobs\n",
    "\n",
    "Ci sono casi in cui conviene eseguire step multipli su un singolo input per fornire l'output desiderato.\n",
    "\n",
    "Se si vogliono creare step multipli, anzichè creare un metodo mapper e reducer, si andrà a specificare quali step eseguire tramite il metodo ``steps()`` che verrà chiamato in automatico da MrJob per eseguire metodi custom invece che i due classici utilizzati in precedenza.\n",
    "\n",
    "Nel prossimo esempio creeremo un processo MrJob per scoprire le parole piu frequenti all'interno del testo. Il primo step sarà esattamente identico a quello del wordcounter, successivamente filtreremo le parole per selezionare la piu usata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_REGEXP = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRMostFreqWord(MRJob):\n",
    "    # Dichiaro i vari Step\n",
    "    # Una lista di oggetti MRStep()\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_wordcount,\n",
    "                    reducer=self.reducer_wordcount),\n",
    "            MRStep(mapper=self.mapper_freq,\n",
    "                    reducer=self.reducer_freq),\n",
    "            MRStep(mapper=self.mapper_most,\n",
    "                    reducer=self.reducer_most)\n",
    "        ]\n",
    "\n",
    "    def mapper_wordcount(self, _, line):\n",
    "        words = WORD_REGEXP.findall(line)\n",
    "        # Conto le parole di lunghezza maggiore a 3\n",
    "        for w in words:\n",
    "            if len(w)>3:\n",
    "                yield w.lower(), 1\n",
    "\n",
    "    def reducer_wordcount(self, word, counts):\n",
    "        yield word, sum(counts)  # Conto le occorrenze di ogni parola per ricavare la frequenza\n",
    "\n",
    "    def mapper_freq(self, word, total):\n",
    "        if total > 1:  # Valuto solo le parole che compaiono piu di una volta\n",
    "            yield total, word  # Le raggruppo per numero di occorrenze\n",
    "\n",
    "    def reducer_freq(self, total, words):\n",
    "        # Avra' all'interno di total il numero di occorrenze\n",
    "        # e all'interno di words un generatore che resituira le singole parole\n",
    "        yield total, words.next()  # .next() restituisce il primo elemento, emetto quindi una sola parola per ogni frequenza\n",
    "\n",
    "    def mapper_most(self, freq, word):\n",
    "        yield 'most_used', [freq, word]  # Raggruppo le parole per una lista di tuple (frequenza, parola)\n",
    "\n",
    "    def reducer_most(self, _, freqs):\n",
    "        yield 'most_used', max(freqs)  # restituisco il max della lista di tuple\n",
    "        # in questo caso il max verra calcolato su tutti i primi elementi delle tuple\n",
    "        # di conseguenza il max() dara come risultato la frequenza piu alta e la parola associata\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        MRMostFreqWord.run()\n",
    "    except TypeError:\n",
    "        print 'MrJob cannot work inside iPython Notebook as it is not saved as a standalone .py file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siccome stiamo eseguendo uno processo multi step avremo un output del tipo:\n",
    "\n",
    "    Creating temp directory /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/script.alexcomu.20161011.132906.601852\n",
    "    Running step 1 of 3...\n",
    "    Running step 2 of 3...\n",
    "    Running step 3 of 3...\n",
    "\n",
    "E avremo come risultato:\n",
    "\n",
    "```\n",
    "\"most_used\"\t[4, \"vitae\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Play! \n",
    "\n",
    "### Esercizio 1\n",
    "\n",
    "Utilizzando il file **lorem.txt** calcolare quante parole iniziano per ciascuna lettera dell'alfabeto.\n",
    "\n",
    "### Esercizio 2\n",
    "\n",
    "Utilizzando il file **lorem.txt** calcolare quante parole iniziano per ciascuna lettera dell'alfabeto e contare il max e il min.\n",
    "\n",
    "### Esercizio 3\n",
    "\n",
    "Scrivere un processo MapReduce che restituisca un report delle parole piu frequenti raggruppati per lunghezza della parola.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "```\n",
    "5 Caratteri -> hello -> 8 occorrenze\n",
    "3 Caratteri -> cat -> 4 occorrenze\n",
    "2 Caratteri -> at -> 7 occorrenze\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eseguire job programmaticamente\n",
    "\n",
    "Eseguire i job da linea di comando è un ottimo modo per testare i nostri algoritmi, molto spesso però è necessario visualizzare i dati calcolati. Dobbiamo quindi essere in grado di eseguire job MapReduce direttamente dal nostro software, gestire quindi input / output e inviare gli eventuali risultati al layer HTML per la visualizzazione.\n",
    "\n",
    "Tutto questo è fattibile tramite i **MrJob Runners**, i quali permettono l'esecuzione programmatica di job MapReduce. Attenzione sempre a mantenere in due file separati **.py** il processo MrJob e il runner.\n",
    "\n",
    "Quindi il runner sarà all'interno della nostra soluzione software, mentre la classe MrJob sarà un modulo separato che potrà essere inviato ad Hadoop per l'esecuzione.\n",
    "\n",
    "Ecco un esempio di runner per il nostro WordFreqCount:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcount import MRWordFreqCount\n",
    "\n",
    "mr_job = MRWordFreqCount()\n",
    "mr_job.stdin = open('lorem.txt').readlines()\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        key, value = mr_job.parse_output_line(line)\n",
    "        print 'Word:', key, 'Count:', value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il parametro ``stdin`` verrà utilizzato per immagazzinare i file di input.\n",
    "Siccome Hadoop gestisce l'input come linee separate abbiamo bisogno di passare una lista di stringhe, motivo per cui usiamo il metodo **readlines()**.\n",
    "\n",
    "Il runner metterà a disposizione il metodo ``stream_output``, il quale restituirà un generatore che ritornerà come singolo elemtno un output Hadoop Streaming. L'output dovrà essere analizzato secondo il protocollo, quindi dovremo chiamare la funzione ``parse_output_line`` **chiave** e **valore**.\n",
    "\n",
    "A questo punto potremo utilizzare i valore come meglio preferiamo, nel nostro caso li stiamo stampando in output.\n",
    "Si può notare che inoltre l'output non avrà nessun log derivato da MrJob ma sarà solo il risultato finale dell'operazione.\n",
    "\n",
    "Quindi se faremo partire il runner senza stampare nulla non avremo nessun output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio - Atleti olimpiadi\n",
    "\n",
    "\n",
    "Struttura del dataset:\n",
    "\n",
    "    age,birthdate,gender,height,name,weight,gold_medals,silver_medals,bronze_medals,total_medals,sport,country\n",
    "\n",
    "Esempio di record:\n",
    "\n",
    "    17,1996-04-12,Male,1.72,Aaron Blunck,68,0,0,0,0,Freestyle Skiing,United States\n",
    "    \n",
    "## Esercizio 4\n",
    "\n",
    "Utilizzando il dataset, calcolare per ogni età il numero di atleti.\n",
    "\n",
    "## Esercizio 5\n",
    "\n",
    "Partendo dall'esercizio precedente, restituire come output l'eta' con il numero maggiore di occorrenze.\n",
    "\n",
    "## Esercizio 6\n",
    "\n",
    "Calcolare il numero di uomini e donne per ogni eta'.\n",
    "\n",
    "## Esercizio 7\n",
    "\n",
    "Calcolare per ogni nazione il numero di medaglie totali e per ciascuna tipologia, rispettivamente per oro, argento e bronzo. Successivamente stampare in output la nazione con piu medaglie in totale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - MovieData DB\n",
    "\n",
    "* Download MovieData DB (100K) from http://grouplens.org/datasets/movielens/\n",
    "\n",
    "I already downloaded the dataset inside the folder ``/PATH/TO/MRJOB/ROOT/examples/_dataset``. Unzip the folder and let's start!\n",
    "\n",
    "We have (from ``u.info`` file):\n",
    "\n",
    "    943 users\n",
    "    1682 films\n",
    "    100000 ratings\n",
    "    \n",
    "We'll use the file u.data which contains (splitted by TAB):\n",
    "\n",
    "    user id | film id | rating | timestamp\n",
    "       299     144        4      877881320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 8 - Rating Counter\n",
    "\n",
    "Count occurrences of rating value from movie DB.\n",
    "\n",
    "### Exercise 9 - Most Rated Movie\n",
    "\n",
    "Count occurrences of each movie rating from movie DB and find the most rated.\n",
    "\n",
    "### Exercise 10 - Quick Lookup\n",
    "\n",
    "Add to the exercise 2 the information about the movie. (use file ``u.item``)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Fake Friends DB\n",
    "\n",
    "Inside folder ``/PATH/TO/MRJOB/ROOT/examples/_dataset`` you will find a csv file called ``fakefriends.csv``. Inside this file there is fake list of users with the relative friends. This is the format:\n",
    "\n",
    "    ID, Name, Age, Number of Friends\n",
    "    0,Will,33,385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11 - User with max friends\n",
    "\n",
    "Find the user which has the MAX number of friends and to the same for the MIN.\n",
    "\n",
    "### Exercise 12 - Friends Avarage per Age\n",
    "\n",
    "Calculate For each Age the Avarage of friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
