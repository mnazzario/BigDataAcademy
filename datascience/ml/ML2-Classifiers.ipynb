{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"http://www.bigdive.eu/wp-content/uploads/2012/05/logoBIGDIVE-01.png\">\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "# Classification Models\n",
    "\n",
    "## AndrÃ© Panisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification datset\n",
    "\n",
    "In the following, we generated 50 samples from a bivariate Gaussian distribution $\\mathcal{ N } ((2, 0)^T , I)$ and labeled this class\n",
    "**RED**. Similarly, 50 more were drawn from $\\mathcal{N} ((0, 2)^T , I)$ and labeled class **GREEN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "nr_samples = 100\n",
    "\n",
    "# Generate 100 samples from two bivariate Gaussian distributions\n",
    "samples_red   = np.random.multivariate_normal(mean=(0,2), cov=np.identity(2)*0.3, size=nr_samples/2)\n",
    "samples_green = np.random.multivariate_normal(mean=(2,0), cov=np.identity(2)*0.3, size=nr_samples/2)\n",
    "\n",
    "# Join the red and green datasets as X and the class definitions as y\n",
    "X = np.concatenate([samples_red, samples_green])\n",
    "y = np.zeros(nr_samples, dtype=int)\n",
    "y[nr_samples/2:] = 1\n",
    "\n",
    "# plot the red and green class points\n",
    "figure(num=None, figsize=(5, 5))\n",
    "plot(samples_red[:,0], samples_red[:,1], 'ro')\n",
    "plot(samples_green[:,0], samples_green[:,1], 'go');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a colormap for the points\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def plot_decision_boundary(model, X, plot_hyperplanes=False, plot_boundary=False):\n",
    "    \"\"\"\n",
    "    Plots the decision boundary of a classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a mesh of points that cover the full graph area\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # use the classifier to predict the class of each mesh point\n",
    "    try:\n",
    "        Z = model.decision_function(X_grid)\n",
    "    except:\n",
    "        Z = model.predict_proba(X_grid)[:,1]-0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    figure(figsize=(8, 6))\n",
    "\n",
    "    # plot the decision boundary\n",
    "    norm = plt.cm.colors.Normalize(vmax=abs(Z).max(), vmin=-abs(Z).max())\n",
    "    contourf(xx, yy, Z, 100, cmap=plt.cm.RdBu, alpha=.8, norm=norm)\n",
    "    xlim(x_min, x_max)\n",
    "    ylim(y_min, y_max)\n",
    "    \n",
    "    if plot_boundary:\n",
    "        # plot the decision hyper-planes\n",
    "        contour(xx, yy, Z, colors=['k'],\n",
    "                linestyles=['-'],\n",
    "                levels=[0])\n",
    "    \n",
    "    if plot_hyperplanes:\n",
    "        # plot the decision hyper-planes\n",
    "        contour(xx, yy, Z, colors=['k', 'k', 'k'],\n",
    "                linestyles=['--', '-', '--'],\n",
    "                levels=[-1., 0, 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "The `Perceptron` is a simple algorithm suitable for large scale learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.Perceptron()\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X, plot_boundary=True)\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression, despite its name, is a linear model for classification\n",
    "rather than regression. Logistic regression is also known in the literature as\n",
    "logit regression, maximum-entropy classification (MaxEnt)\n",
    "or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a [logistic function](http://en.wikipedia.org/wiki/Logistic_function>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X)\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes methods are a set of supervised learning algorithms\n",
    "based on applying Bayes' theorem with the \"naive\" assumption of independence\n",
    "between every pair of features. Given a class variable $y$ and a\n",
    "dependent feature vector $x_1$ through $x_n$,\n",
    "Bayes' theorem states the following relationship:\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                    {P(x_1, \\dots, x_n)}$$\n",
    "\n",
    "Using the naive independence assumption that\n",
    "\n",
    "$$ P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y), $$\n",
    "\n",
    "for all $i$, this relationship is simplified to\n",
    "\n",
    "$$ P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                    {P(x_1, \\dots, x_n)} $$\n",
    "\n",
    "Since $P(x_1, \\dots, x_n)$ is constant given the input,\n",
    "we can use the following classification rule:\n",
    "\n",
    "$$ P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y) $$\n",
    "$$   \\Downarrow $$\n",
    "$$   \\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y), $$\n",
    "\n",
    "and we can use Maximum A Posteriori (MAP) estimation to estimate\n",
    "$P(y)$ and $P(x_i \\mid y)$;\n",
    "the former is then the relative frequency of class $y$\n",
    "in the training set.\n",
    "\n",
    "The different naive Bayes classifiers differ mainly by the assumptions they\n",
    "make regarding the distribution of $P(x_i \\mid y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = naive_bayes.GaussianNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X)\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decition Trees\n",
    "\n",
    "**Decision Trees (DTs)** are a set of supervised learning models used\n",
    "for `classification <tree_classification>` and `regression`.\n",
    "The goal is to create a model that predicts the value of a\n",
    "target variable by learning simple decision rules inferred from the data\n",
    "features.\n",
    "\n",
    "Some advantages of decision trees are:\n",
    "\n",
    "    - Simple to understand and to interpret. Trees can be visualised.\n",
    "\n",
    "    - Requires little data preparation. Other techniques often require data\n",
    "      normalisation, dummy variables need to be created and blank values to\n",
    "      be removed. Note however that this module does not support missing\n",
    "      values.\n",
    "\n",
    "    - The cost of using the tree (i.e., predicting data) is logarithmic in the\n",
    "      number of data points used to train the tree.\n",
    "\n",
    "    - Able to handle both numerical and categorical data. Other techniques\n",
    "      are usually specialised in analysing datasets that have only one type\n",
    "      of variable. See :ref:`algorithms <tree_algorithms>` for more\n",
    "      information.\n",
    "\n",
    "    - Able to handle multi-output problems.\n",
    "\n",
    "    - Uses a white box model. If a given situation is observable in a model,\n",
    "      the explanation for the condition is easily explained by boolean logic.\n",
    "      By contrast, in a black box model (e.g., in an artificial neural\n",
    "      network), results may be more difficult to interpret.\n",
    "\n",
    "    - Possible to validate a model using statistical tests. That makes it\n",
    "      possible to account for the reliability of the model.\n",
    "\n",
    "    - Performs well even if its assumptions are somewhat violated by\n",
    "      the true model from which the data were generated.\n",
    "\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "\n",
    "    - Decision-tree learners can create over-complex trees that do not\n",
    "      generalise the data well. This is called overfitting. Mechanisms\n",
    "      such as pruning (not currently supported), setting the minimum\n",
    "      number of samples required at a leaf node or setting the maximum\n",
    "      depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "    - Decision trees can be unstable because small variations in the\n",
    "      data might result in a completely different tree being generated.\n",
    "      This problem is mitigated by using decision trees within an\n",
    "      ensemble.\n",
    "\n",
    "    - The problem of learning an optimal decision tree is known to be\n",
    "      NP-complete under several aspects of optimality and even for simple\n",
    "      concepts. Consequently, practical decision-tree learning algorithms\n",
    "      are based on heuristic algorithms such as the greedy algorithm where\n",
    "      locally optimal decisions are made at each node. Such algorithms\n",
    "      cannot guarantee to return the globally optimal decision tree.  This\n",
    "      can be mitigated by training multiple trees in an ensemble learner,\n",
    "      where the features and samples are randomly sampled with replacement.\n",
    "\n",
    "    - There are concepts that are hard to learn because decision trees\n",
    "      do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "\n",
    "    - Decision tree learners create biased trees if some classes dominate.\n",
    "      It is therefore recommended to balance the dataset prior to fitting\n",
    "      with the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X)\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "**Support vector machines (SVMs)** are a set of supervised learning\n",
    "models used mainly for `classification`, but it can be used also for `regression`.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "    - Effective in high dimensional spaces.\n",
    "\n",
    "    - Still effective in cases where number of dimensions is greater\n",
    "      than the number of samples.\n",
    "\n",
    "    - Uses a subset of training points in the decision function (called\n",
    "      support vectors), so it is also memory efficient.\n",
    "\n",
    "    - Versatile: different `svm_kernels` can be\n",
    "      specified for the decision function. Common kernels are\n",
    "      provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "    - If the number of features is much greater than the number of\n",
    "      samples, the method is likely to give poor performances.\n",
    "\n",
    "    - SVMs do not directly provide probability estimates, these are\n",
    "      calculated using an expensive five-fold cross-validation.\n",
    "\n",
    "A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n",
    "\n",
    "For a linear SVM classifier, the parameter **C** controls the penalty of the error term for a soft-margin SVM classifier. Setting **C** to a high value means that the classifier will try to maximize the margin with a high penalty for misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC(kernel='linear', C=10.)\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X, plot_hyperplanes=True)\n",
    "\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\n",
    "\n",
    "# plot the support vectors\n",
    "SV = model.support_vectors_\n",
    "scatter(SV[:, 0], SV[:, 1], c=y[model.support_],\n",
    "        cmap=cm_bright, s=300, marker='o', facecolors='none', linewidths=2.);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='poly', degree=10, C=10.)\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X, plot_hyperplanes=True)\n",
    "\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\n",
    "\n",
    "# plot the support vectors\n",
    "SV = model.support_vectors_\n",
    "scatter(SV[:, 0], SV[:, 1], c=y[model.support_],\n",
    "        cmap=cm_bright, s=300, marker='o', facecolors='none', linewidths=2.);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf', gamma=1e-2, C=100.)\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X, plot_hyperplanes=True)\n",
    "\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\n",
    "\n",
    "# plot the support vectors\n",
    "SV = model.support_vectors_\n",
    "scatter(SV[:, 0], SV[:, 1], c=y[model.support_],\n",
    "        cmap=cm_bright, s=300, marker='o', facecolors='none', linewidths=2.);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf', gamma=10., C=1.)\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X, plot_hyperplanes=True)\n",
    "\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)\n",
    "\n",
    "# plot the support vectors\n",
    "SV = model.support_vectors_\n",
    "scatter(SV[:, 0], SV[:, 1], c=y[model.support_],\n",
    "        cmap=cm_bright, s=300, marker='o', facecolors='none', linewidths=2.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods: Random Forests\n",
    "\n",
    "The goal of **ensemble methods** is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "\n",
    "In **random forests**, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, **the bias of the forest usually slightly increases** (with respect to the bias of a single non-random tree) but, due to averaging, **its variance also decreases**, usually more than compensating for the increase in bias, hence yielding an overall better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "model = ensemble.RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "model.fit(X, y)\n",
    "\n",
    "plot_decision_boundary(model, X)\n",
    "\n",
    "# plot the dataset points\n",
    "scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Find the best model to classify the digits dataset.\n",
    "\n",
    "Use a 10-fold cross-validation for the model selection phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the first 21 samples\n",
    "for index, (image, label) in enumerate(zip(digits.images, digits.target)[:21]):\n",
    "    plt.subplot(3, 7, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('%i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
