{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"http://www.bigdive.eu/wp-content/uploads/2012/05/logoBIGDIVE-01.png\">\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "# Scikit-Learn: Introduction\n",
    "\n",
    "## André Panisson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a collection of tools for machine learning written in Python:\n",
    "[http://scikit-learn.org](http://scikit-learn.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of Data in Scikit-learn\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays.\n",
    "\n",
    "\n",
    "A classification algorithm, for example, expects the data to be represented as a **feature matrix** and a **label vector**:\n",
    "\n",
    "$$\n",
    "{\\rm feature~matrix:~~~} {\\bf X}~=~\\left[\n",
    "\\begin{matrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1D}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2D}\\\\\n",
    "x_{31} & x_{32} & \\cdots & x_{3D}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{N1} & x_{N2} & \\cdots & x_{ND}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\rm label~vector:~~~} {\\bf y}~=~ [y_1, y_2, y_3, \\cdots y_N]\n",
    "$$\n",
    "\n",
    "Here there are $N$ samples and $D$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: the Iris Dataset\n",
    "\n",
    "As an example of a simple dataset, we're going to take a look at the\n",
    "iris data stored by scikit-learn.\n",
    "The data consists of measurements of three different species of irises.\n",
    "There are three species of iris in the dataset, which we can picture here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "display(Image('http://mirlab.org/jang/books/dcpr/image/Iris-setosa-10_1.jpg', width=200, height=200))\n",
    "print \"Iris Setosa\\n\"\n",
    "\n",
    "display(Image('http://mirlab.org/jang/books/dcpr/image/Iris-versicolor-21_1.jpg', width=200, height=200))\n",
    "print \"Iris Versicolor\\n\"\n",
    "\n",
    "display(Image('http://mirlab.org/jang/books/dcpr/image/Iris-virginica-3_1.jpg', width=200, height=200))\n",
    "print \"Iris Virginica\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Iris Data with Scikit-Learn\n",
    "\n",
    "Scikit-learn has a very straightforward set of data on these iris species.  The data consist of\n",
    "the following:\n",
    "\n",
    "- Features in the Iris dataset:\n",
    "\n",
    "  1. sepal length in cm\n",
    "  2. sepal width in cm\n",
    "  3. petal length in cm\n",
    "  4. petal width in cm\n",
    "\n",
    "- Target classes to predict:\n",
    "\n",
    "  1. Iris Setosa\n",
    "  2. Iris Versicolour\n",
    "  3. Iris Virginica\n",
    "  \n",
    "``scikit-learn`` embeds a copy of the iris CSV file along with a helper function to load it into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print iris['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "print (n_samples, n_features)\n",
    "print iris.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print iris.data.shape\n",
    "print iris.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is four dimensional, but we can visualize two of the dimensions\n",
    "at a time using a simple scatter-plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_index = 2\n",
    "y_index = 3\n",
    "\n",
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index],\n",
    "            c=iris.target)\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = pd.Series(iris.target).map(lambda c: iris.target_names[c])\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.PairGrid(iris_df, hue='target')\n",
    "g.map_diag(plt.hist)\n",
    "g.map_offdiag(plt.scatter)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Available Data\n",
    "They come in three flavors:\n",
    "\n",
    "- **Packaged Data:** these small datasets are packaged with the scikit-learn installation,\n",
    "  and can be downloaded using the tools in ``sklearn.datasets.load_*``\n",
    "- **Downloadable Data:** these larger datasets are available for download, and scikit-learn\n",
    "  includes tools which streamline this process.  These tools can be found in\n",
    "  ``sklearn.datasets.fetch_*``\n",
    "- **Generated Data:** there are several datasets which are generated from models based on a\n",
    "  random seed.  These are available in the ``sklearn.datasets.make_*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets.make_moons(n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Digits Dataset**: This dataset is composed of 1797 images of $8 \\times 8$ pixels. You have to transform it in a matrix of 1797 samples and 64 features ($1797 \\times 64$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the first 21 samples\n",
    "for index, (image, label) in enumerate(zip(digits.images, digits.target)[:21]):\n",
    "    plt.subplot(3, 7, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('%i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Scikit-learn Estimator Object\n",
    "\n",
    "Every algorithm is exposed in scikit-learn via an ''Estimator'' object. For instance a linear regression is implemented as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator parameters**: All the parameters of an estimator can be set when it is instantiated, and have suitable default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(normalize=True, fit_intercept=False)\n",
    "print model.normalize\n",
    "print model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimated Model parameters**: When data is *fit* with an estimator, parameters are estimated from the data at hand. All the estimated parameters are attributes of the estimator object ending by an underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.rand(100)*10\n",
    "y = x + np.random.randn(x.shape[0])\n",
    "\n",
    "plt.plot(x, y, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The input data for sklearn is 2D: (samples == 3 x features == 1)\n",
    "X = x[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X, y) \n",
    "print model.coef_\n",
    "print model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict([[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration of the Bias-Variance Tradeoff with Scikit-Learn\n",
    "\n",
    "For this section, we'll work with a simple 1D regression problem.  This will help us to\n",
    "easily visualize the data and the model, and the results generalize easily to  higher-dimensional\n",
    "datasets.  We'll explore a simple **linear regression** problem.\n",
    "This can be accomplished within scikit-learn with the `sklearn.linear_model` module.\n",
    "\n",
    "We'll create a simple nonlinear function that we'd like to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x, err=0.5):\n",
    "    y = 10 - 1. / (x + 0.1)\n",
    "    if err > 0:\n",
    "        y = np.random.normal(y, err)\n",
    "    return y\n",
    "\n",
    "def make_data(N=40, error=1.0, random_seed=1):\n",
    "    # randomly sample the data\n",
    "    np.random.seed(random_seed)\n",
    "    X = np.random.random(N)[:, np.newaxis]\n",
    "    y = f(X.ravel(), error)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40, error=1)\n",
    "plt.scatter(X.ravel(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = make_data(400, error=1.0, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "X_train = X[:N]\n",
    "y_train = y[:N]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_train.ravel(), y_train)\n",
    "plt.scatter(X_test.ravel(), y_pred, color='r')\n",
    "print \"in-sample mean squared error:\", metrics.mean_squared_error(model.predict(X_train), y_train)\n",
    "print \"out-of-sample mean squared error:\", metrics.mean_squared_error(model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have fit a straight line to the data, but clearly this model is not a good choice.  We say that this model is **biased**, or that it **under-fits** the data.\n",
    "\n",
    "Let's try to improve this by creating a more complicated model.  We can do this by adding degrees of freedom, and computing a polynomial regression over the inputs.  Let's make this easier by creating a quick PolynomialRegression estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolynomialRegression(LinearRegression):\n",
    "    \"\"\"Simple Polynomial Regression to 1D data\"\"\"\n",
    "    def __init__(self, degree=1, **kwargs):\n",
    "        self.degree = degree\n",
    "        LinearRegression.__init__(self, **kwargs)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if X.shape[1] != 1:\n",
    "            raise ValueError(\"Only 1D data valid here\")\n",
    "        Xp = X ** (1 + np.arange(self.degree))\n",
    "        return LinearRegression.fit(self, Xp, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Xp = X ** (1 + np.arange(self.degree))\n",
    "        return LinearRegression.predict(self, Xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "X_train = X[:N]\n",
    "y_train = y[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(degree=2)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X_train.ravel(), y_train)\n",
    "plt.scatter(X_test.ravel(), y_pred, color='r')\n",
    "print \"in-sample mean squared error:\", metrics.mean_squared_error(model.predict(X_train), y_train)\n",
    "print \"out-of-sample mean squared error:\", metrics.mean_squared_error(model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves: Number of Training Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values of $E_{in}$ and $E_{out}$ for different values of $N$, with a simple model with degree 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(degree=2)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "training_error = []\n",
    "test_error = []\n",
    "mse = metrics.mean_squared_error\n",
    "\n",
    "N_list = range(2, 40)\n",
    "\n",
    "for N in N_list:\n",
    "    X_train = X[:N]\n",
    "    y_train = y[:N]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    training_error.append(mse(model.predict(X_train), y_train))\n",
    "    test_error.append(mse(model.predict(X_test), y_test))\n",
    "    \n",
    "plt.plot(N_list, training_error, label='$E_{in}$')\n",
    "plt.plot(N_list, test_error, label='$E_{out}$')\n",
    "plt.legend()\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('MSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the degree to 3, we add one more parameter and have a more complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Create a Polynomical Regression model with degree 3.\n",
    "\n",
    "Plot the learning curves with values of $E_{in}$ and $E_{out}$ for different values of $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare $E_{in}$ and $E_{out}$ to the Polynomial Regression with degree 2. Which model would you choose for $N=30$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the analysis with a Polynomical Regression model with degree 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves: Model Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of learning curve is shown below.\n",
    "\n",
    "We have a fixed number of training samples N (e.g., 20). We plot the values of $E_{in}$ and $E_{out}$ for different levels of model complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "X_train = X[:N]\n",
    "y_train = y[:N]\n",
    "\n",
    "degrees = np.arange(1, 20)\n",
    "\n",
    "training_error = []\n",
    "test_error = []\n",
    "mse = metrics.mean_squared_error\n",
    "\n",
    "for d in degrees:\n",
    "    model = PolynomialRegression(d).fit(X_train, y_train)\n",
    "    training_error.append(mse(model.predict(X_train), y_train))\n",
    "    test_error.append(mse(model.predict(X_test), y_test))\n",
    "    \n",
    "plt.plot(degrees, training_error, label='$E_{in}$')\n",
    "plt.plot(degrees, test_error, label='$E_{out}$')\n",
    "plt.legend()\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.ylim(0, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Increase the number of training samples (e.g., 40) and plot the values of $E_{in}$ and $E_{out}$ for different levels of model complexity.\n",
    "\n",
    "Would you choose the same model complexity as if it was just 20 training samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From RMSE to Coefficient of Determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Sklearn, most of the times we will be interested in the **score** of the model, not in the error.\n",
    "\n",
    "An example of model score to evaluate a linear regression is the $R^2$ (coefficient of determination):\n",
    "\n",
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=0}^{n_{\\text{samples}} - 1} (y_i - \\hat{y}_i)^2}{\\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "where $\\bar{y} =  \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}} - 1} y_i$.\n",
    "\n",
    "The best possible score is 1.0, lower values are worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "X_train = X[:N]\n",
    "y_train = y[:N]\n",
    "\n",
    "degrees = np.arange(1, 20)\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "r2 = metrics.r2_score\n",
    "\n",
    "for d in degrees:\n",
    "    model = PolynomialRegression(d).fit(X_train, y_train)\n",
    "    training_scores.append(r2(model.predict(X_train), y_train))\n",
    "    test_scores.append(r2(model.predict(X_test), y_test))\n",
    "\n",
    "plt.plot(degrees, training_scores, label='in-sample score')\n",
    "plt.plot(degrees, test_scores, label='out-of-sample score')\n",
    "plt.legend()\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.ylim(0.5, 1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Check the performance of different models in terms of Coefficient of Determination with 40 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation\n",
    "\n",
    "\n",
    "\n",
    "### Cross validation\n",
    "\n",
    "Cross-validation (CV) is a model validation technique for assessing how the results of a model will generalize to an independent data set (i.e., it is used to estimate the value of $E_{out}$).\n",
    "\n",
    "In the basic CV approach, called *k*-fold CV,\n",
    "the training set is split into *k* smaller sets.\n",
    "The following procedure is followed for each of the *k* \"folds\":\n",
    "\n",
    " * A model is trained using $k-1$ of the folds as training data;\n",
    " * the resulting model is validated on the remaining part of the data\n",
    "   (i.e., it is used as a test set to compute a performance measure\n",
    "   such as accuracy).\n",
    "\n",
    "The performance measure reported by *k*-fold cross-validation\n",
    "is then the average of the values computed in the loop.\n",
    "This approach can be computationally expensive,\n",
    "but does not waste too much data\n",
    "(as it is the case when fixing an arbitrary test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(5)\n",
    "\n",
    "np.mean(cross_validation.cross_val_score(model, X_train, y_train, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "X_train = X[:N]\n",
    "y_train = y[:N]\n",
    "\n",
    "degrees = np.arange(2, 16)\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "r2 = metrics.r2_score\n",
    "\n",
    "for d in degrees:\n",
    "    model = PolynomialRegression(d).fit(X_train, y_train)\n",
    "    training_scores.append(r2(model.predict(X_train), y_train))\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(model, X_train, y_train, cv=5)\n",
    "    test_scores.append(np.mean(scores))\n",
    "\n",
    "# note that the test error can also be computed via cross-validation\n",
    "plt.plot(degrees, training_scores, label='training score')\n",
    "plt.plot(degrees, test_scores, label='cross-validated score')\n",
    "plt.legend()\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.ylim(0.0, 1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Use numpy to select the hyperparameter (polynomial degree) that gives the best cross-validated score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "With regularization, we add constraints to the model, by limiting the space of the parameter values.\n",
    "\n",
    "For example, with L1 regularization, we add a term to the error function:\n",
    "$$\\alpha \\sum_{d=0}^D {w_d}$$\n",
    "With L2 regularization, we add this term:\n",
    "$$\\alpha \\sum_{d=0}^D {w_d^2} $$\n",
    "For example, the error function of Linear Regression with L1 regularization becomes\n",
    "$$ \\frac{1}{N} \\sum_{i=1}^N {\\left( \\mathbf{w}x_i - y_i \\right)} + \\alpha \\sum_{d=0}^D {w_d} $$\n",
    "With regularization, we are not free to change all parameters independently: If we set one parameter to a high value, we implicitly force other parameters to be smaller. The degrees of freedom thus are smaller than without the restriction. As result, we are reducing the effective parameters (or VC dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "class RegularizedPolynomialRegression(Lasso):\n",
    "    \"\"\"Simple Polynomial Regression to 1D data\"\"\"\n",
    "    def __init__(self, degree=1, alpha=1.0, **kwargs):\n",
    "        self.degree = degree\n",
    "        self.alpha = alpha\n",
    "        Lasso.__init__(self, alpha=alpha, **kwargs)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if X.shape[1] != 1:\n",
    "            raise ValueError(\"Only 1D data valid here\")\n",
    "        Xp = X ** (1 + np.arange(self.degree))\n",
    "        return Lasso.fit(self, Xp, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Xp = X ** (1 + np.arange(self.degree))\n",
    "        return Lasso.predict(self, Xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = make_data(30, error=1)\n",
    "\n",
    "degree = 1000\n",
    "alphas = np.logspace(-4, -2, 50)\n",
    "\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "r2 = metrics.r2_score\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = RegularizedPolynomialRegression(degree, alpha=alpha).fit(X_train, y_train)\n",
    "    training_scores.append(r2(model.predict(X_train), y_train))\n",
    "    \n",
    "    scores = cross_validation.cross_val_score(model, X_train, y_train, cv=5)\n",
    "    test_scores.append(np.mean(scores))\n",
    "\n",
    "# note that the test error can also be computed via cross-validation\n",
    "plt.plot(alphas, training_scores, label='training score')\n",
    "plt.plot(alphas, test_scores, label='cross-validated score')\n",
    "plt.legend()\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xscale('log')\n",
    "plt.ylim(np.min(test_scores), 1.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Use numpy to select the hyperparameter (alpha) that gives the best cross-validated score.\n",
    "\n",
    "Then plot the test points with their true values and their predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search and model selection\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "The grid search provided by **GridSearchCV** exhaustively generates candidates from a grid of parameter values specified with the `param_grid` parameter.\n",
    "\n",
    "The GridSearchCV implements the usual estimator API: when “fitting” it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "X_train = X[:N]\n",
    "y_train = y[:N]\n",
    "\n",
    "degrees = np.arange(2, 16)\n",
    "\n",
    "model = PolynomialRegression(2)\n",
    "\n",
    "param_grid = {\"degree\": degrees}\n",
    "gs = grid_search.GridSearchCV(model, param_grid)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
